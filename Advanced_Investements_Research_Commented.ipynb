{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3d91e3",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced Investments Research â€” Commented Notebook\n",
    "\n",
    "This version adds clear explanations **before each step**, plus **inline comments** in code cells.\n",
    "It is designed to be **GitHub-ready** and easy to follow for collaborators and reviewers.\n",
    "\n",
    "**How to run**\n",
    "1. Ensure the required packages are installed (see `requirements.txt`).\n",
    "2. Open this notebook, run cells in order. If you load data from Google Drive, make sure the link has access permissions.\n",
    "3. At the end, export results (CSV/plots) for your report.\n",
    "\n",
    "> Tip: For reproducibility, avoid editing data in-place without keeping an original copy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076455a0",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Rolling window calculations (e.g., moving averages / betas)\n",
    "# Run rolling OLS to estimate time-varying relationships\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fee75e",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "# Load a CSV from Google Drive link; low_memory=False avoids dtype guesses on large files\n",
    "data_path_1= 'https://drive.google.com/file/d/1gRNNfzKBtJc6jeEPRQpT_BhA-L0XX5lv/view?usp=share_link'\n",
    "file_id = '1gRNNfzKBtJc6jeEPRQpT_BhA-L0XX5lv'\n",
    "url= 'https://drive.google.com/uc?id=1gRNNfzKBtJc6jeEPRQpT_BhA-L0XX5lv'\n",
    "data_1 = pd.read_csv(url, header=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ccc40",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27791ab4",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4749c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d457bf",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "data=data_1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff6ec7",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "# Use a MultiIndex (ticker, date) so we can perform panel operations efficiently\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "# Extract only the month (as an integer)\n",
    "data['date'] = data['date'].dt.strftime('%Y-%m')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "stock_returns = data[['date', 'tickers', 'ret_exc']]\n",
    "stock_returns = stock_returns.drop_duplicates(subset=['date', 'tickers'])\n",
    "data.set_index(['tickers', 'date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ebb44",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Drop rows with missing values created during transformations\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Define columns to process\n",
    "columns_to_check = ['ret_exc', 'seas_6_10an', 'seas_6_10na', 'cop_at', 'noa_gr1a', 'o_score', 'ival_me', 'resff3_12_1']\n",
    "\n",
    "# Ensure numeric conversion and apply winsorization column-wise\n",
    "for col in columns_to_check:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')  # Convert to numeric\n",
    "    data[col] = winsorize(data[col].values, limits=[0.02, 0.02])  # Winsorize\n",
    "\n",
    "# Interpolate missing values\n",
    "data_filtered = data.interpolate(method='linear')\n",
    "\n",
    "# Drop rows where any required column still has NaN\n",
    "data_filtered = data_filtered.dropna(subset=columns_to_check)\n",
    "\n",
    "# Display result\n",
    "print(data_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab08f5",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba10db1",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c6dbb",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8997cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "tickers=data_filtered.index.levels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d487af",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af90fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd411551",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc45d2c",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab48f60",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2cdcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Load a CSV from Google Drive link; low_memory=False avoids dtype guesses on large files\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# URL of the ZIP file\n",
    "url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip\"\n",
    "\n",
    "# Step 1: Download the ZIP file\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))  # Convert to in-memory ZIP\n",
    "\n",
    "# Step 2: Extract and list files\n",
    "zip_file_contents = zip_file.namelist()\n",
    "print(\"Files inside ZIP:\", zip_file_contents)  # Check file names inside ZIP\n",
    "\n",
    "# Step 3: Read the first CSV (or choose the correct one if multiple)\n",
    "csv_filename = zip_file_contents[0]  # First file in ZIP (check manually if needed)\n",
    "with zip_file.open(csv_filename) as file:\n",
    "    df = pd.read_csv(file, skiprows=3)  # Skip metadata rows if necessary\n",
    "\n",
    "# Step 4: Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6e484",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ede5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "ff_5_monthly=df[:738]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f59bb7",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "ff_5_monthly=ff_5_monthly.rename(columns={'Unnamed: 0': 'date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574bda8",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcde1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "ff_5_monthly['date'] = pd.to_datetime(ff_5_monthly['date'], format='%Y%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8728fe",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "ff_5_monthly[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']] = ff_5_monthly[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180d96d",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eacb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "ff_5_monthly[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]=ff_5_monthly[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]/100\n",
    "ff_5_monthly['RF'] = ff_5_monthly['RF']/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d2b6d",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "ff_5_monthly.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce444b9a",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e67be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization ===\n",
    "# Plot results with matplotlib\n",
    "plt.plot(ff_5_monthly)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331b968",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "# Load a CSV from Google Drive link; low_memory=False avoids dtype guesses on large files\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "\n",
    "## Regress the excess return of the portfolio first on the market beta and then on\n",
    "## the Fama&French 5 factors plus momentum\n",
    "\n",
    "# Upload the Fama&French factors data\n",
    "#FF = pd.read_csv(\"/Users/lucadonadini/Desktop/Magistrale/Secondo anno/Second semester/Advanced Investments/Research project/FF6.csv\", delimiter=',')\n",
    "#FF.rename(columns={'dateff': 'date'}, inplace=True)\n",
    "#FF['date'] = pd.to_datetime(FF['date'])\n",
    "#FF['date'] = FF['date'].dt.strftime('%Y-%m')\n",
    "#FF['date'] = pd.to_datetime(FF['date'])\n",
    "\n",
    "# Merge the two datasets\n",
    "merged_data = pd.merge(data_filtered, ff_5_monthly, on='date', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a1304",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17433b84",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "def min_max_scale_df(df): #scale for pca\n",
    "    \"\"\"\n",
    "    Scale every numeric column in the DataFrame to the range [0, 1].\n",
    "    Non-numeric columns are left unchanged.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame with numeric columns scaled to [0, 1].\n",
    "    \"\"\"\n",
    "    scaled_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if col_max - col_min == 0:\n",
    "                scaled_df[col] = 0.0\n",
    "            else:\n",
    "                scaled_df[col] = (df[col] - col_min) / (col_max - col_min)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb8e2e",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21591064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "merged_data[['seas_6_10an',\t'seas_6_10na'\t,'cop_at',\t'noa_gr1a'\t,'o_score'\t,'ival_me',\t'resff3_12_1'\t,'Mkt-RF'\t,'SMB',\t'HML',\t'RMW',\t'CMA',\t'RF']]=min_max_scale_df(merged_data[['seas_6_10an',\t'seas_6_10na'\t,'cop_at',\t'noa_gr1a'\t,'o_score'\t,'ival_me',\t'resff3_12_1'\t,'Mkt-RF'\t,'SMB',\t'HML',\t'RMW',\t'CMA',\t'RF']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8622c",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization ===\n",
    "# Plot results with matplotlib\n",
    "plt.plot(merged_data[['seas_6_10an',\t'seas_6_10na'\t,'cop_at',\t'noa_gr1a'\t,'o_score'\t,'ival_me',\t'resff3_12_1'\t,'Mkt-RF'\t,'SMB',\t'HML',\t'RMW',\t'CMA',\t'RF']])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b4f37",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75badd",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.stats import gmean\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "    \"\"\"\n",
    "    Generate a positional encoding with exactly 'depth' features.\n",
    "    If necessary, pad the result so that its last dimension is exactly 'depth'.\n",
    "    \"\"\"\n",
    "    # Cast depth for computations.\n",
    "    depth_float = tf.cast(depth, tf.float32)\n",
    "    depth_int = tf.cast(depth, tf.int32)\n",
    "\n",
    "    # Compute half depth (for sin and cos parts).\n",
    "    half_depth = depth_int // 2\n",
    "\n",
    "    # positions: shape (length, 1)\n",
    "    positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]\n",
    "    # depths_range: shape (1, half_depth)\n",
    "    depths_range = tf.range(half_depth, dtype=tf.float32)[tf.newaxis, :]\n",
    "\n",
    "    # Scale depths_range by (depth / 2) so that division is done in float.\n",
    "    denom = depth_float / 2.0\n",
    "    depths_scaled = depths_range / denom  # shape: (1, half_depth)\n",
    "\n",
    "    # Compute the angle rates and angles.\n",
    "    angle_rates = 1 / (10000 ** depths_scaled)  # shape: (1, half_depth)\n",
    "    angle_rads = positions * angle_rates          # shape: (length, half_depth)\n",
    "\n",
    "    # Concatenate sin and cos to get a tensor of shape (length, half_depth*2)\n",
    "    pos_encoding = tf.concat([tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    # Pad if the concatenated dimension is less than the desired depth.\n",
    "    current_depth = tf.shape(pos_encoding)[-1]\n",
    "    pad_size = depth_int - current_depth\n",
    "    pos_encoding = tf.pad(pos_encoding, [[0, 0], [0, pad_size]])\n",
    "\n",
    "    # Finally, slice to ensure the last dimension is exactly depth.\n",
    "    pos_encoding = pos_encoding[:, :depth_int]\n",
    "\n",
    "    return pos_encoding  # shape: (length, depth)\n",
    "\n",
    "class CnnPositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, length, d_model=None):\n",
    "        \"\"\"\n",
    "        length: maximum sequence length.\n",
    "        d_model: if provided, this fixed feature dimension is used;\n",
    "                 otherwise, the layer infers the input's feature dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.length = length\n",
    "        self.d_model = d_model  # if None, will use input's last dimension\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        # Determine number of features: use d_model if provided; otherwise, infer from x.\n",
    "        if self.d_model is not None:\n",
    "            num_features = self.d_model\n",
    "        else:\n",
    "            num_features = x.shape[-1] if x.shape[-1] is not None else tf.shape(x)[-1]\n",
    "\n",
    "        # Generate positional encoding with exactly num_features features.\n",
    "        pos_encoding = positional_encoding(seq_length, num_features)\n",
    "        # pos_encoding shape: (seq_length, num_features)\n",
    "\n",
    "        # Expand and tile to shape (batch_size, seq_length, num_features)\n",
    "        pos_encoding = tf.expand_dims(pos_encoding, 0)\n",
    "        pos_encoding = tf.tile(pos_encoding, [batch_size, 1, 1])\n",
    "\n",
    "        return x + pos_encoding\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, value=x, key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x, key=context, value=context, return_attention_scores=True)\n",
    "        self.last_attn_scores = attn_scores  # Cache scores for later (if needed)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, value=x, key=x, use_causal_mask=True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "        self.ffn = FeedForward(d_model, dff, dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, length, num_layers, d_model, num_heads, dff, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        # Here, we force the encoder to operate in a space of dimension d_model.\n",
    "        self.CnnPositionalEncoding = CnnPositionalEncoding(length=length, d_model=d_model)\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.CnnPositionalEncoding(x)  # Add positional encoding.\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "        return x  # Output shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "class Lstm(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_data, target_dim):\n",
    "        \"\"\"\n",
    "        d_data: number of features for the LSTM input.\n",
    "        target_dim: final output dimension (e.g. 473 if that's your target dimension).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(d_data, activation='tanh', recurrent_activation='tanh',\n",
    "                                 use_bias=True, return_sequences=True, dropout=0.1),\n",
    "            tf.keras.layers.LSTM(target_dim, activation='tanh', recurrent_activation='tanh',\n",
    "                                 use_bias=True, return_sequences=True, dropout=0.1),\n",
    "            tf.keras.layers.Dense(target_dim, activation='tanh')\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.seq(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e89f0",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "class LSTM_Encoder(tf.keras.Model):\n",
    "  def __init__(self, *,length,d_data,target_dim,\n",
    "               num_layers, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.lstm= Lstm(d_data=d_data,target_dim=target_dim)\n",
    "      #self.cnn = CNN(length=length,d_data=d_data,Filters=Filters,kernel=kernel)\n",
    "      self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           length=length,\n",
    "                           dropout_rate=dropout_rate)\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate) #d_model: 20*(14+1)*8 cumulative residuals are also a feature\n",
    "      #self.final_layer_1 = tf.keras.layers.Dense(40,activation='relu')\n",
    "      #self.final_layer_2 = tf.keras.layers.Dense(d_model,activation='tanh')\n",
    "      #self.final_layer_3 = tf.keras.layers.Dense(d_model,activation='tanh')\n",
    "      #self.final_layer_4 = tf.keras.layers.Dense(d_model/14*5*2,activation='tanh')\n",
    "      self.output_layer =tf.keras.layers.Dense(target_dim,activation='tanh')\n",
    "      #self.softmax_layer=tf.keras.layers.Softmax() #default is last axis, axis need to be checked\n",
    "  def call(self, inputs):\n",
    "      x=inputs\n",
    "      x=self.lstm(x)\n",
    "      #x=self.cnn(x) #How does the model sequentially!!!\n",
    "      x=self.encoder(x)  # (batch_size, context_len, d_model)\n",
    "      #x=self.final_layer_1(x)\n",
    "      x=self.output_layer(x) #(x[:,-1,:])\n",
    "\n",
    "      try:\n",
    "          del x._keras_mask\n",
    "      except AttributeError:\n",
    "          pass\n",
    "      return x#logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e6211",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "class GeometricSharpeRatioLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Calculate portfolio return using predicted weights and actual returns\n",
    "        portfolio_return = tf.reduce_sum(y_pred * y_true, axis=1)\n",
    "\n",
    "        # Handling edge cases\n",
    "        portfolio_return = tf.where(tf.math.is_nan(portfolio_return), tf.zeros_like(portfolio_return), portfolio_return)\n",
    "\n",
    "        # Calculate geometric mean\n",
    "        geometric_mean = tf.reduce_prod(1 + portfolio_return, axis=0) ** (1.0 / tf.cast(tf.shape(portfolio_return)[0], tf.float32))\n",
    "\n",
    "        # Handling edge cases\n",
    "        geometric_mean = tf.where(tf.math.is_nan(geometric_mean), tf.ones_like(geometric_mean), geometric_mean)\n",
    "        portfolio_volatility=(tf.math.reduce_std(portfolio_return, axis=0)+ 1e-10)\n",
    "\n",
    "        # Calculate Sharpe ratio\n",
    "        #sharpe_ratio = geometric_mean / portfolio_volatility\n",
    "        sharpe_ratio = portfolio_return / portfolio_volatility\n",
    "\n",
    "        # Handling edge cases\n",
    "        sharpe_ratio = tf.where(tf.math.is_nan(sharpe_ratio), tf.ones_like(sharpe_ratio), sharpe_ratio)\n",
    "\n",
    "        # Calculate negative geometric mean of daily Sharpe ratios\n",
    "        return -sharpe_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b7cae",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataset_train(data, window_size, stride, batch_size, num_features):\n",
    "    x, y, index = [], [], []\n",
    "    for i in range(0, data.shape[0] - window_size, stride):\n",
    "        x.append(data.iloc[i : i + window_size].values)\n",
    "        y.append(data.iloc[i + window_size].values)\n",
    "        index.append(data.index[i + window_size])\n",
    "\n",
    "    x, y, index = np.array(x), np.array(y), np.array(index)\n",
    "\n",
    "    # Ensure batch size is a multiple of batch_size\n",
    "    total_size = x.shape[0]\n",
    "    new_size = batch_size * (total_size // batch_size)\n",
    "    start_size = total_size - new_size\n",
    "    x = x[start_size:].reshape(new_size, window_size, num_features)\n",
    "    y = y[start_size:].reshape(new_size, num_features)\n",
    "    index = index[start_size:].reshape(new_size, 1)\n",
    "    index_flat = index.ravel()\n",
    "    index=pd.to_datetime(index_flat, format='%Y-%m-%d')\n",
    "\n",
    "    return x, y, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19211ae2",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "def create_features_test(data, window_size, stride, batch_size, num_features):\n",
    "    x, y, index = [], [], []\n",
    "    for i in range(0, data.shape[0] - window_size, stride):\n",
    "        x.append(data.iloc[i+1 : i + window_size+1].values)\n",
    "        index.append(data.index[i + window_size])\n",
    "\n",
    "    x, y, index = np.array(x), np.array(y), np.array(index)\n",
    "\n",
    "    # Ensure batch size is a multiple of batch_size\n",
    "    total_size = x.shape[0]\n",
    "    new_size = batch_size * (total_size // batch_size)\n",
    "    start_size = total_size - new_size\n",
    "    x = x[start_size:].reshape(new_size, window_size, num_features)\n",
    "    #y = y[start_size:].reshape(new_size, num_features)\n",
    "    index = index[start_size:].reshape(new_size, 1)\n",
    "    index_flat = index.ravel()\n",
    "    index=pd.to_datetime(index_flat, format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "    return x,index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da872b",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "def create_dataset_train(data, window_size, stride, batch_size, num_features):\n",
    "    x, y, index = [], [], []\n",
    "    for i in range(0, data.shape[0] - window_size, stride):\n",
    "        x.append(data.iloc[i : i + window_size].values)\n",
    "        y.append(data.iloc[i + window_size].values)\n",
    "        index.append(data.index[i + window_size])\n",
    "\n",
    "    x, y, index = np.array(x), np.array(y), np.array(index)\n",
    "\n",
    "    # Ensure batch size is a multiple of batch_size\n",
    "    total_size = x.shape[0]\n",
    "    new_size = batch_size * (total_size // batch_size)\n",
    "    start_size = total_size - new_size\n",
    "    x = x[start_size:].reshape(new_size, window_size, num_features)\n",
    "    y = y[start_size:].reshape(new_size, num_features)\n",
    "    index = index[start_size:].reshape(new_size, 1)\n",
    "    index_flat = index.ravel()\n",
    "    index=pd.to_datetime(index_flat, format='%Y-%m-%d')\n",
    "\n",
    "    return x, y, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703b539",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "def create_dataset_train_modified(data, window_size, stride, batch_size, num_features):\n",
    "    x,index = [], []\n",
    "    for i in range(0, data.shape[0] - window_size, stride):\n",
    "        x.append(data.iloc[i : i + window_size].values)\n",
    "        index.append(data.index[i + window_size])\n",
    "\n",
    "    x, index = np.array(x), np.array(index)\n",
    "\n",
    "    # Ensure batch size is a multiple of batch_size\n",
    "    total_size = x.shape[0]\n",
    "    new_size = batch_size * (total_size // batch_size)\n",
    "    start_size = total_size - new_size\n",
    "    x = x[start_size:].reshape(new_size, window_size, num_features)\n",
    "    index = index[start_size:].reshape(new_size, 1)\n",
    "    index_flat = index.ravel()\n",
    "    index=pd.to_datetime(index_flat, format='%Y-%m-%d')\n",
    "\n",
    "    return x,index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1fc685",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "def create_features_test(data, window_size, stride, batch_size, num_features):\n",
    "    x, y, index = [], [], []\n",
    "    for i in range(0, data.shape[0] - window_size, stride):\n",
    "        x.append(data.iloc[i+1 : i + window_size+1].values)\n",
    "        index.append(data.index[i + window_size])\n",
    "\n",
    "    x, y, index = np.array(x), np.array(y), np.array(index)\n",
    "\n",
    "    # Ensure batch size is a multiple of batch_size\n",
    "    total_size = x.shape[0]\n",
    "    new_size = batch_size * (total_size // batch_size)\n",
    "    start_size = total_size - new_size\n",
    "    x = x[start_size:].reshape(new_size, window_size, num_features)\n",
    "    #y = y[start_size:].reshape(new_size, num_features)\n",
    "    index = index[start_size:].reshape(new_size, 1)\n",
    "    index_flat = index.ravel()\n",
    "    index=pd.to_datetime(index_flat, format='%Y-%m-%d')\n",
    "\n",
    "    return x,index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f34a51",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d463a12",
   "metadata": {},
   "source": [
    "### Feature engineering / rolling stats\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature engineering / rolling stats ===\n",
    "# Group by entity (e.g., ticker) to compute stats per asset\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Step 1: Filter out gvkeys with fewer than 100 observations\n",
    "gvkey_counts = merged_data['gvkey'].value_counts()\n",
    "valid_gvkeys = gvkey_counts[gvkey_counts >= 100].index  # Keep only gvkeys with 100+ rows\n",
    "filtered_data = merged_data[merged_data['gvkey'].isin(valid_gvkeys)]  # Remove small gvkeys\n",
    "\n",
    "# Step 2: Dictionary to store train-test sets separately for each gvkey\n",
    "train_sets = {}\n",
    "test_sets = {}\n",
    "\n",
    "# Step 3: Loop through each gvkey and create separate train-test splits\n",
    "for gvkey, group in filtered_data.groupby(\"gvkey\"):\n",
    "    group = group.sort_index()  # Ensure the data is sorted by date\n",
    "\n",
    "    # Drop duplicate dates for this gvkey, keeping only the first occurrence\n",
    "    group = group[~group.index.duplicated(keep='first')]\n",
    "\n",
    "    # Compute split index\n",
    "    split_idx = int(len(group) * train_ratio)\n",
    "\n",
    "    # Train and test split\n",
    "    train_sets[gvkey] = group.iloc[:split_idx].drop(columns=['gvkey'])  # First 80% for training\n",
    "    test_sets[gvkey] = group.iloc[split_idx:].drop(columns=['gvkey'])   # Last 20% for testing\n",
    "\n",
    "# Step 4: Example - Accessing train set for a specific gvkey\n",
    "example_gvkey = list(train_sets.keys())[0]\n",
    "\n",
    "# Step 5: Check if gvkeys with < 100 observations are removed\n",
    "print(\"Unique gvkeys in train sets:\", len(train_sets))\n",
    "print(\"Unique gvkeys in test sets:\", len(test_sets))\n",
    "print(f\"Train set for gvkey {example_gvkey}:\\n\", train_sets[example_gvkey].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6be64",
   "metadata": {},
   "source": [
    "### Feature engineering / rolling stats\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79257995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature engineering / rolling stats ===\n",
    "# Group by entity (e.g., ticker) to compute stats per asset\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Step 1: Filter out gvkeys with fewer than 100 observations\n",
    "gvkey_counts = merged_data['gvkey'].value_counts()\n",
    "valid_gvkeys = gvkey_counts[gvkey_counts >= 100].index  # Keep only gvkeys with 100+ rows\n",
    "filtered_data = merged_data[merged_data['gvkey'].isin(valid_gvkeys)]\n",
    "\n",
    "# Optional: Sort the entire filtered dataset by its index (e.g., date)\n",
    "filtered_data = filtered_data.sort_index()\n",
    "filtered_data = filtered_data.drop(columns=['Unnamed: 0'])\n",
    "# Step 2: Dictionary to store train-test sets separately for each gvkey\n",
    "train_sets = {}\n",
    "test_sets = {}\n",
    "\n",
    "# Step 3: Loop through each gvkey and create separate train-test splits\n",
    "for gvkey, group in filtered_data.groupby(\"gvkey\"):\n",
    "    # Ensure the group's index is sorted (e.g., by date)\n",
    "    group = group.sort_index()\n",
    "\n",
    "    # Drop duplicate dates for this gvkey, keeping only the first occurrence\n",
    "    group = group[~group.index.duplicated(keep='first')]\n",
    "\n",
    "    # Compute split index\n",
    "    split_idx = int(len(group) * train_ratio)\n",
    "\n",
    "    # Train and test split and then sort each split by the index\n",
    "    train = group.iloc[:split_idx].drop(columns=['gvkey']).sort_index()\n",
    "    test = group.iloc[split_idx:].drop(columns=['gvkey']).sort_index()\n",
    "\n",
    "    train_sets[gvkey] = train\n",
    "    test_sets[gvkey] = test\n",
    "\n",
    "# Step 4: Example - Accessing train set for a specific gvkey\n",
    "example_gvkey = list(train_sets.keys())[0]\n",
    "\n",
    "# Step 5: Check if gvkeys with < 100 observations are removed\n",
    "print(\"Unique gvkeys in train sets:\", len(train_sets))\n",
    "print(\"Unique gvkeys in test sets:\", len(test_sets))\n",
    "print(f\"Train set for gvkey {example_gvkey}:\\n\", train_sets[example_gvkey].head())\n",
    "training_set=pd.concat(train_sets,axis=1)\n",
    "test_set=pd.concat(test_sets,axis=1)\n",
    "training_set=training_set[:-15].interpolate(method='linear').fillna(0)\n",
    "test_set=test_set.interpolate(method='linear').fillna(0)\n",
    "training_set.columns = training_set.columns.droplevel(0)\n",
    "test_set.columns = test_set.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd16ff",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66226125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1aaef",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b07e89",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "target_set_test=test_set.loc[:, test_set.columns == 'ret_exc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fb9f9",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af57ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "target_set_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f28fa",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce16d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def min_max_scale_df(df):\n",
    "    \"\"\"\n",
    "    Scale every numeric column in the DataFrame to the range [0, 1].\n",
    "    Non-numeric columns are left unchanged.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame with numeric columns scaled to [0, 1].\n",
    "    \"\"\"\n",
    "    scaled_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if col_max - col_min == 0:\n",
    "                scaled_df[col] = 0.0\n",
    "            else:\n",
    "                scaled_df[col] = (df[col] - col_min) / (col_max - col_min)\n",
    "    return scaled_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame:\n",
    "#s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86287f",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "target_set=training_set.loc[:, training_set.columns == 'ret_exc'].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c13937",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ac5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "target_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c534be",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "target_set.columns=tickers[:442]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94051af",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Drop rows with missing values created during transformations\n",
    "target_set.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60437ee8",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "#target_set_test=test_set.loc[:, test_set.columns == 'ret_exc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0d1da",
   "metadata": {},
   "source": [
    "### Inspection / display\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e075ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inspection / display ===\n",
    "def create_dataset_target(data, window_size, stride, batch_size):\n",
    "    y, index = [], []\n",
    "\n",
    "    # Step 1: Collect full window targets instead of just last value\n",
    "    for i in range(0, data.shape[0] - window_size, stride):\n",
    "        y.append(data.iloc[i : i + window_size].values)  # Store all targets in the window\n",
    "        index.append(data.index[i : i + window_size])  # Store corresponding indices\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    y, index = np.array(y), np.array(index)\n",
    "\n",
    "    # Step 2: Ensure batch size alignment\n",
    "    total_size = y.shape[0]\n",
    "    new_size = batch_size * (total_size // batch_size)  # Ensure divisible by batch_size\n",
    "    start_size = total_size - new_size\n",
    "\n",
    "    print(f\"Original Target Shape: {y.shape}\")  # Debugging check\n",
    "\n",
    "    # Step 3: Reshape to (batch_size, window_size, 1)\n",
    "    y = y[start_size:].reshape(new_size, window_size, 1)\n",
    "\n",
    "    return y, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6390920",
   "metadata": {},
   "source": [
    "### Inspection / display\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inspection / display ===\n",
    "def create_dataset_target(data, window_size, stride, batch_size):\n",
    "    y, index = [], []\n",
    "\n",
    "    # Step 1: Collect full window targets (including features)\n",
    "    for i in range(0, data.shape[0] - window_size, stride):\n",
    "        y.append(data.iloc[i : i + window_size].values)  # Store all targets in the window\n",
    "        index.append(data.index[i : i + window_size])  # Store corresponding indices\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    y, index = np.array(y), np.array(index)\n",
    "\n",
    "    # Step 2: Ensure batch size alignment\n",
    "    total_size = y.shape[0]\n",
    "    new_size = batch_size * (total_size // batch_size)  # Ensure divisible by batch_size\n",
    "    start_size = total_size - new_size\n",
    "\n",
    "    print(f\"Original Target Shape: {y.shape}\")  # Debugging check\n",
    "\n",
    "    # Step 3: Reshape to (batch_size, window_size, num_features)\n",
    "    num_features = data.shape[1]  # Extract number of features\n",
    "    y = y[start_size:].reshape(new_size, window_size, num_features)\n",
    "\n",
    "    return y, index[start_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc35e9b",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd53576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Parse 'date' column into pandas datetime for time-series operations\n",
    "def create_dataset_train_modified(data, window_size, stride, batch_size, num_features):\n",
    "    x,index = [], []\n",
    "    for i in range(0, data.shape[0] - window_size, stride):\n",
    "        x.append(data.iloc[i : i + window_size].values)\n",
    "        index.append(data.index[i + window_size])\n",
    "\n",
    "    x, index = np.array(x), np.array(index)\n",
    "\n",
    "    # Ensure batch size is a multiple of batch_size\n",
    "    total_size = x.shape[0]\n",
    "    new_size = batch_size * (total_size // batch_size)\n",
    "    start_size = total_size - new_size\n",
    "    x = x[start_size:].reshape(new_size, window_size, num_features)\n",
    "    index = index[start_size:].reshape(new_size, 1)\n",
    "    index_flat = index.ravel()\n",
    "    index=pd.to_datetime(index_flat, format='%Y-%m-%d')\n",
    "\n",
    "    return x,index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27730731",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70487cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "intersection_index=training_set.index.intersection(target_set.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292f5cf",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e762730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "intersection_index_test=test_set.index.intersection(target_set_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589b83d",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d380d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "intersection_index_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3252d",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309df4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "#target_set_test=target_set_test.loc[intersection_index_test].interpolate(method='linear').fillna(0)\n",
    "#test_set=test_set.loc[intersection_index_test].interpolate(method='linear').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef29487",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a69cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "training_tensor,training_index=create_dataset_train_modified(training_set, 12, 1, 12, training_set.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f5c63",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd171a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "test_tensor,test_index=create_dataset_train_modified(test_set, 12, 1, 12, test_set.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbb99c",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "training_set.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2638df1",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example: df is your DataFrame with a DateTimeIndex and duplicate columns.\n",
    "# For instance, df.columns might be:\n",
    "# ['ret_exc', 'seas_6_10an', 'seas_6_10na', 'cop_at', 'noa_gr1a', 'o_score', 'ival_me',\n",
    "#  'resff3_12_1', 'Mkt-RF', 'SMB', ..., 'noa_gr1a', 'o_score', 'ival_me', 'resff3_12_1',\n",
    "#  'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "\n",
    "# First, count how many times each column name appears.\n",
    "col_counts = training_set.columns.value_counts()\n",
    "\n",
    "# Filter to only include columns that have at least 5 duplicates.\n",
    "cols_to_process = [col for col, count in col_counts.items() if count >= 5]\n",
    "\n",
    "pca_results = []  # list to store the PCA DataFrames\n",
    "\n",
    "for col in cols_to_process:\n",
    "    # Select all columns with the given name.\n",
    "    sub_df = training_set.loc[:, training_set.columns == col]\n",
    "\n",
    "    # Optional: fill NaNs (you might choose a different strategy)\n",
    "    sub_df = sub_df.fillna(0)\n",
    "\n",
    "    # Initialize PCA to extract 5 components.\n",
    "    pca = PCA(n_components=5)\n",
    "\n",
    "    # Fit and transform the sub-dataframe.\n",
    "    # sub_df has shape (n_samples, n_duplicates)\n",
    "    pca_scores = pca.fit_transform(sub_df)\n",
    "\n",
    "    # Create a DataFrame for the PCA scores.\n",
    "    # Name the new columns as e.g., \"noa_gr1a_pc1\", \"noa_gr1a_pc2\", ...\n",
    "    pca_df = pd.DataFrame(\n",
    "        pca_scores,\n",
    "        index=sub_df.index,\n",
    "        columns=[f\"{col}_pc{i+1}\" for i in range(5)]\n",
    "    )\n",
    "\n",
    "    pca_results.append(pca_df)\n",
    "\n",
    "# Concatenate the PCA results along the columns to create the final dataset.\n",
    "final_df = pd.concat(pca_results, axis=1)\n",
    "\n",
    "# Optionally, inspect the resulting DataFrame:\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407e3d1",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example: df is your DataFrame with a DateTimeIndex and duplicate columns.\n",
    "# For instance, df.columns might be:\n",
    "# ['ret_exc', 'seas_6_10an', 'seas_6_10na', 'cop_at', 'noa_gr1a', 'o_score', 'ival_me',\n",
    "#  'resff3_12_1', 'Mkt-RF', 'SMB', ..., 'noa_gr1a', 'o_score', 'ival_me', 'resff3_12_1',\n",
    "#  'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "\n",
    "# First, count how many times each column name appears.\n",
    "col_counts = test_set.columns.value_counts()\n",
    "\n",
    "# Filter to only include columns that have at least 5 duplicates.\n",
    "cols_to_process = [col for col, count in col_counts.items() if count >= 5]\n",
    "\n",
    "pca_results = []  # list to store the PCA DataFrames\n",
    "\n",
    "for col in cols_to_process:\n",
    "    # Select all columns with the given name.\n",
    "    sub_df = test_set.loc[:, training_set.columns == col]\n",
    "\n",
    "    # Optional: fill NaNs (you might choose a different strategy)\n",
    "    sub_df = sub_df.fillna(0)\n",
    "\n",
    "    # Initialize PCA to extract 5 components.\n",
    "    pca = PCA(n_components=5)\n",
    "\n",
    "    # Fit and transform the sub-dataframe.\n",
    "    # sub_df has shape (n_samples, n_duplicates)\n",
    "    pca_scores = pca.fit_transform(sub_df)\n",
    "\n",
    "    # Create a DataFrame for the PCA scores.\n",
    "    # Name the new columns as e.g., \"noa_gr1a_pc1\", \"noa_gr1a_pc2\", ...\n",
    "    pca_df = pd.DataFrame(\n",
    "        pca_scores,\n",
    "        index=sub_df.index,\n",
    "        columns=[f\"{col}_pc{i+1}\" for i in range(5)]\n",
    "    )\n",
    "\n",
    "    pca_results.append(pca_df)\n",
    "\n",
    "# Concatenate the PCA results along the columns to create the final dataset.\n",
    "final_df_test = pd.concat(pca_results, axis=1)\n",
    "\n",
    "# Optionally, inspect the resulting DataFrame:\n",
    "print(final_df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57865bcf",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c08196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79535398",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "final_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708688df",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc504621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "#scaled_df = min_max_scale_df(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2182e1",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fa3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6298c",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed09285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Plot results with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot each feature\n",
    "for column in final_df.columns:\n",
    "    plt.plot(final_df.index, final_df[column], label=column, linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Improve aesthetics\n",
    "plt.title(\"Neural Network Features Over Time for Training Set\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Time\", fontsize=14)\n",
    "plt.ylabel(\"Feature Value\", fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Adjust the legend to fit inside the plot neatly\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1), ncol=2, fontsize=10, frameon=False)\n",
    "\n",
    "# Optimize layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a2214",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be051f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Plot results with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot each feature\n",
    "for column in final_df_test.columns:\n",
    "    plt.plot(final_df_test.index, final_df_test[column], label=column, linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Improve aesthetics\n",
    "plt.title(\"Neural Network Features Over Time for Test Set\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Time\", fontsize=14)\n",
    "plt.ylabel(\"Feature Value\", fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Adjust the legend to fit inside the plot neatly\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1), ncol=2, fontsize=10, frameon=False)\n",
    "\n",
    "# Optimize layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb5422",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fd470",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094f7c0",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "training_tensor,training_index=create_dataset_train_modified(final_df, 12, 1, 12, final_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279cbc2",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4958613",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df327884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "test_tensor,test_index=create_dataset_train_modified(final_df_test, 12, 1, 12, final_df_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f91b946",
   "metadata": {},
   "source": [
    "_(Markdown cell left intentionally blank in the original. Kept for structure.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546f1ea",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21914a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "target_tensor,target_index=create_dataset_target(target_set, 12, 1, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9f19d",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f58dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "training_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7a15c",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "training_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7268502",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c17133",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a690f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import tensorflow as tf\n",
    "\n",
    "class SharpeRatioLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, risk_free_rate=0.0, epsilon=1e-3):\n",
    "        super().__init__()\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Optional normalization if needed (e.g., softmax for weights)\n",
    "        # y_pred = tf.keras.activations.softmax(y_pred, axis=-1)\n",
    "\n",
    "        # Ensure y_pred is 3D\n",
    "        if len(y_pred.shape) == 2:\n",
    "            y_pred = tf.expand_dims(y_pred, axis=-1)\n",
    "\n",
    "        if y_pred.shape[-1] != y_true.shape[-1]:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch: y_pred has {y_pred.shape[-1]} assets, but y_true has {y_true.shape[-1]} assets.\"\n",
    "            )\n",
    "\n",
    "        # Compute portfolio returns and clip extreme values\n",
    "        portfolio_returns = tf.reduce_sum(y_pred * y_true, axis=2)\n",
    "        portfolio_returns = tf.clip_by_value(portfolio_returns, -1e3, 1e3)\n",
    "        tf.debugging.check_numerics(portfolio_returns, message=\"Portfolio returns contain NaNs\")\n",
    "\n",
    "        # Mean return\n",
    "        mean_return = tf.reduce_mean(portfolio_returns, axis=1)\n",
    "        # Standard deviation (avoid division by zero)\n",
    "        std_dev = tf.math.reduce_std(portfolio_returns, axis=1)\n",
    "        std_dev_safe = tf.where(tf.less(std_dev, self.epsilon), self.epsilon, std_dev)\n",
    "        tf.debugging.check_numerics(std_dev_safe, message=\"Standard deviation (safe) contains NaNs\")\n",
    "\n",
    "        # Compute Sharpe ratio\n",
    "        sharpe_ratio = mean_return / std_dev_safe\n",
    "        sharpe_ratio = tf.clip_by_value(sharpe_ratio, -1e3, 1e3)\n",
    "        tf.debugging.check_numerics(sharpe_ratio, message=\"Sharpe ratio contains NaNs\")\n",
    "\n",
    "        loss = -tf.reduce_mean(sharpe_ratio)\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca469cb2",
   "metadata": {},
   "source": [
    "### Inspection / display\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inspection / display ===\n",
    "def contains_nan(tensor):\n",
    "    \"\"\"Return True if the tensor contains any NaNs, else False.\"\"\"\n",
    "    return tf.reduce_any(tf.math.is_nan(tensor))\n",
    "\n",
    "# Check training_tensor\n",
    "if contains_nan(training_tensor):\n",
    "    print(\"training_tensor contains NaNs.\")\n",
    "else:\n",
    "    print(\"training_tensor does not contain NaNs.\")\n",
    "\n",
    "# Check target_tensor\n",
    "if contains_nan(target_tensor):\n",
    "    print(\"target_tensor contains NaNs.\")\n",
    "else:\n",
    "    print(\"target_tensor does not contain NaNs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff94321",
   "metadata": {},
   "source": [
    "### Inspection / display\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inspection / display ===\n",
    "if contains_nan(training_tensor):\n",
    "    print(\"training_tensor contains NaNs.\")\n",
    "else:\n",
    "    print(\"training_tensor does not contain NaNs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691c892",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c561d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "models = {}\n",
    "learning_rate = 1e-5  # Lower learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "\n",
    "model = LSTM_Encoder(\n",
    "    length=12,\n",
    "    d_data=training_tensor.shape[2],\n",
    "    d_model=target_tensor.shape[2],  # target_dim, e.g., 473\n",
    "    target_dim=target_tensor.shape[2],\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    dff=target_tensor.shape[2] * 4,  # e.g., 4x target_dim\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=SharpeRatioLoss(epsilon=1e-3))\n",
    "model.fit(training_tensor, target_tensor, epochs=10, batch_size=12)\n",
    "models['arch'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624bbdc",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c586a51",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions on the test set.\n",
    "# This will output a tensor of shape (num_samples, seq_length, target_dim)\n",
    "predictions = model.predict(test_tensor)\n",
    "\n",
    "# Option 1: Use the predictions from the last time step.\n",
    "final_predictions = predictions[:, -1, :]  # shape: (num_samples, target_dim)\n",
    "\n",
    "# Option 2: (Alternate) Average the predictions over the sequence.\n",
    "# final_predictions = np.mean(predictions, axis=1)  # shape: (num_samples, target_dim)\n",
    "\n",
    "# Now, suppose each column corresponds to a stock.\n",
    "# For each sample, we can rank the stocks by sorting the weights in descending order.\n",
    "# For example, for the first sample:\n",
    "sample_index = 0\n",
    "stock_weights = final_predictions[sample_index]  # shape: (target_dim,)\n",
    "# Get ranking indices (highest weight first)\n",
    "ranking_indices = np.argsort(-stock_weights)\n",
    "print(\"For sample\", sample_index, \"the ranking of stocks (by index) is:\")\n",
    "print(ranking_indices)\n",
    "\n",
    "# If you want an overall ranking across the test set,\n",
    "# you could average the weights over all samples.\n",
    "average_weights = np.mean(final_predictions, axis=0)  # shape: (target_dim,)\n",
    "overall_ranking = np.argsort(-average_weights)\n",
    "print(\"Overall ranking of stocks (by index):\")\n",
    "print(overall_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7a09e",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5dfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions on the test set.\n",
    "# This will output a tensor of shape (num_samples, seq_length, target_dim)\n",
    "predictions = model.predict(test_tensor)\n",
    "\n",
    "# Option 1: Use the predictions from the last time step.\n",
    "final_predictions = predictions[:, -1, :]  # shape: (num_samples, target_dim)\n",
    "\n",
    "# Option 2: (Alternate) Average the predictions over the sequence.\n",
    "# final_predictions = np.mean(predictions, axis=1)  # shape: (num_samples, target_dim)\n",
    "\n",
    "# Now, suppose each column corresponds to a stock.\n",
    "# For each sample, we can rank the stocks by sorting the weights in descending order.\n",
    "# For example, for the first sample:\n",
    "#stock_weights = final_predictions[test_index]  # shape: (target_dim,)\n",
    "# Get ranking indices (highest weight first)\n",
    "#ranking_indices = np.argsort(-stock_weights)\n",
    "#print(\"For sample\", test_index, \"the ranking of stocks (by index) is:\")\n",
    "#print(ranking_indices)\n",
    "\n",
    "# If you want an overall ranking across the test set,\n",
    "# you could average the weights over all samples.\n",
    "#average_weights = np.mean(final_predictions, axis=0)  # shape: (target_dim,)\n",
    "#overall_ranking = np.argsort(-average_weights)\n",
    "#print(\"Overall ranking of stocks (by index):\")\n",
    "#print(overall_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728df854",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afb41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "final_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1423d19",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "predictions_df=pd.DataFrame(final_predictions,columns=tickers[:442],index=test_index[-12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20855aaf",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d03fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c7cd5",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Drop rows with missing values created during transformations\n",
    "def create_decile_portfolios(pred_df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of predictions with dates as the index and tickers as columns,\n",
    "    this function creates decile portfolios by ranking the predictions on each date.\n",
    "\n",
    "    It returns:\n",
    "      - decile_assignments: DataFrame with the same shape as pred_df where each cell\n",
    "                            contains the decile group (1 to 10) for that ticker on that date.\n",
    "      - decile_means: DataFrame with dates as the index and decile numbers (1 to 10) as columns,\n",
    "                      where each cell is the average predicted value for that decile group on that date.\n",
    "    \"\"\"\n",
    "    # Create empty DataFrames to store decile assignments and decile means.\n",
    "    decile_assignments = pd.DataFrame(index=pred_df.index, columns=pred_df.columns)\n",
    "    decile_means = pd.DataFrame(index=pred_df.index, columns=range(1, 11))\n",
    "\n",
    "    # Loop through each date (row) in the prediction DataFrame.\n",
    "    for date in pred_df.index:\n",
    "        # Get the predictions for the date; drop NaN values if any.\n",
    "        row = pred_df.loc[date].dropna()\n",
    "\n",
    "        # Check if there are enough unique values to create 10 deciles.\n",
    "        if row.nunique() < 10:\n",
    "            # If not, assign NaN decile for this date.\n",
    "            deciles = pd.Series(np.nan, index=row.index)\n",
    "        else:\n",
    "            # Use pd.qcut to assign decile groups (1 to 10); lowest values get decile 1.\n",
    "            deciles = pd.qcut(row, 10, labels=False) + 1\n",
    "\n",
    "        # Fill in the decile assignments for this date.\n",
    "        decile_assignments.loc[date, deciles.index] = deciles\n",
    "\n",
    "        # Calculate the average prediction for each decile group.\n",
    "        for dec in range(1, 11):\n",
    "            # Select stocks in the current decile.\n",
    "            group_values = row[deciles == dec]\n",
    "            # Compute the mean for this decile; if no stocks, set as NaN.\n",
    "            decile_means.loc[date, dec] = group_values.mean() if not group_values.empty else np.nan\n",
    "\n",
    "    return decile_assignments, decile_means\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Suppose pred_df is your DataFrame of predictions.\n",
    "# For example:\n",
    "# pred_df =\n",
    "# tickers       A      AAPL    ABBV   ABNB   ...   ZBRA    ZTS\n",
    "# 2017-12-01 0.391412 -0.955903 0.377957 -0.633543 ...  -0.819471 0.179298\n",
    "# 2018-01-01 0.391199 -0.955949 0.377955 -0.633269 ...  -0.819547 0.179085\n",
    "# ...\n",
    "\n",
    "# Create decile portfolios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e8967",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Drop rows with missing values created during transformations\n",
    "def get_decile_stock_lists(pred_df):\n",
    "    \"\"\"\n",
    "    For each date (row) in the predictions DataFrame, rank the stocks and\n",
    "    split them into deciles (1 to 10). Returns a dictionary where each key is a date\n",
    "    and its value is a dictionary mapping decile numbers (1-10) to a list of stock tickers\n",
    "    that fall into that decile.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pred_df : pd.DataFrame\n",
    "        DataFrame with dates as index and tickers as columns, containing predictions.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of the form:\n",
    "        { date1: {1: [tickerA, tickerB, ...],\n",
    "                   2: [tickerC, tickerD, ...],\n",
    "                   ...,\n",
    "                   10: [...] },\n",
    "          date2: { ... },\n",
    "          ... }\n",
    "    \"\"\"\n",
    "    decile_dict = {}\n",
    "\n",
    "    for date in pred_df.index:\n",
    "        # Get predictions for this date as a Series (ticker names are the index)\n",
    "        row = pred_df.loc[date].dropna()\n",
    "\n",
    "        # Check if there are enough unique values to create deciles\n",
    "        if row.nunique() < 10:\n",
    "            # Not enough unique values: assign empty lists for each decile.\n",
    "            decile_assignments = {d: [] for d in range(1, 11)}\n",
    "        else:\n",
    "            # Use pd.qcut to divide the ranked values into 10 equal groups\n",
    "            # labels=False returns integers 0-9; add 1 to make deciles 1-10.\n",
    "            decile_labels = pd.qcut(row, 10, labels=False) + 1\n",
    "\n",
    "            # Create a dictionary mapping decile number to the list of tickers in that decile.\n",
    "            decile_assignments = {}\n",
    "            for d in range(1, 11):\n",
    "                tickers_in_decile = row.index[decile_labels == d].tolist()\n",
    "                decile_assignments[d] = tickers_in_decile\n",
    "\n",
    "        decile_dict[date] = decile_assignments\n",
    "\n",
    "    return decile_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5e6ad",
   "metadata": {},
   "source": [
    "### Inspection / display\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inspection / display ===\n",
    "example_date = predictions_df.index[-1]\n",
    "print(f\"Decile groups for {example_date}:\")\n",
    "for decile, tickers in get_decile_stock_lists(predictions_df)[example_date].items():\n",
    "    print(f\"  Decile {decile}: {tickers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a4524",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "def normalize_df(df):\n",
    "    \"\"\"\n",
    "    Normalizes every numeric column in the DataFrame to the range [0, 1].\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame with each numeric column normalized to [0, 1].\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "\n",
    "    for col in df_norm.columns:\n",
    "        # Process only numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(df_norm[col]):\n",
    "            col_min = df_norm[col].min()\n",
    "            col_max = df_norm[col].max()\n",
    "            # Avoid division by zero for constant columns\n",
    "            if col_max - col_min == 0:\n",
    "                df_norm[col] = 0.0\n",
    "            else:\n",
    "                df_norm[col] = (df_norm[col] - col_min) / (col_max - col_min)\n",
    "\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38deefc4",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Drop rows with missing values created during transformations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def rank_predictions_into_deciles(predictions_df):\n",
    "    \"\"\"\n",
    "    For each date (row) in predictions_df, rank the stocks by their predicted value\n",
    "    and split them into deciles (1 to 10). Returns a dictionary where:\n",
    "      - Keys are dates.\n",
    "      - Values are dictionaries mapping decile number (1 to 10) to a list of tickers\n",
    "        that fall in that decile.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_df : pd.DataFrame\n",
    "        DataFrame with dates as index and ticker symbols as columns.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    decile_dict : dict\n",
    "        Dictionary mapping each date to decile portfolios.\n",
    "    \"\"\"\n",
    "    decile_dict = {}\n",
    "\n",
    "    for date in predictions_df.index:\n",
    "        # Get the predictions for the current date.\n",
    "        row = predictions_df.loc[date].dropna()\n",
    "\n",
    "        # If there are fewer than 10 unique values, we cannot form deciles.\n",
    "        if row.nunique() < 10:\n",
    "            decile_assignments = {d: [] for d in range(1, 11)}\n",
    "        else:\n",
    "            # Use pd.qcut to split the values into 10 equal groups.\n",
    "            # qcut returns labels 0 to 9; we add 1 so that deciles range from 1 to 10.\n",
    "            decile_labels = pd.qcut(row, 10, labels=False) + 1\n",
    "\n",
    "            # Build a dictionary mapping decile number to list of tickers.\n",
    "            decile_assignments = {}\n",
    "            for dec in range(1, 11):\n",
    "                tickers_in_decile = row.index[decile_labels == dec].tolist()\n",
    "                # Optionally, you can sort the tickers by their prediction value:\n",
    "                tickers_in_decile = sorted(tickers_in_decile, key=lambda t: row[t])\n",
    "                decile_assignments[dec] = tickers_in_decile\n",
    "\n",
    "        decile_dict[date] = decile_assignments\n",
    "\n",
    "    return decile_dict\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume predictions_df is your DataFrame of LSTM encoder predictions\n",
    "# with dates as the index and ticker symbols as the columns.\n",
    "# For example:\n",
    "#            AAPL      MSFT     GOOG   ...  ZTS\n",
    "# 2023-01-01  0.12     0.05     -0.03  ...  0.09\n",
    "# 2023-02-01  0.08     0.03      0.01  ...  0.10\n",
    "# ...\n",
    "\n",
    "decile_portfolios = rank_predictions_into_deciles(predictions_df)\n",
    "\n",
    "# To print the decile assignments for a particular date:\n",
    "example_date = predictions_df.index[0]\n",
    "print(f\"Decile portfolios for {example_date}:\")\n",
    "for decile, tickers in decile_portfolios[example_date].items():\n",
    "    print(f\"  Decile {decile}: {tickers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b40fe",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89966038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Drop rows with missing values created during transformations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_DL_factor(predictions_df):\n",
    "    \"\"\"\n",
    "    Compute the DL_Momentum_Factor from LSTM encoder predictions.\n",
    "\n",
    "    For each date, stocks are ranked into deciles. The factor is defined as:\n",
    "\n",
    "        DL_Momentum_Factor = (Average prediction of decile 10) - (Average prediction of decile 1)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_df : pd.DataFrame\n",
    "        DataFrame with dates as index and ticker symbols as columns containing predictions.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with a single column 'DL_Momentum_Factor' and dates as the index.\n",
    "    \"\"\"\n",
    "    factor_values = {}\n",
    "\n",
    "    for date in predictions_df.index:\n",
    "        # Get the predictions for the current date\n",
    "        row = predictions_df.loc[date].dropna()\n",
    "\n",
    "        # Check if there are at least 10 unique values to form deciles\n",
    "        if row.nunique() < 10:\n",
    "            factor_values[date] = np.nan\n",
    "        else:\n",
    "            # Rank the predictions into deciles (1 = bottom, 10 = top)\n",
    "            decile_labels = pd.qcut(row, 10, labels=False) + 1  # Labels: 1 to 10\n",
    "\n",
    "            # Compute mean predictions for decile 10 and decile 1\n",
    "            top_mean = row[decile_labels == 10].mean()\n",
    "            bottom_mean = row[decile_labels == 1].mean()\n",
    "\n",
    "            # Compute the DL_Momentum_Factor as the difference\n",
    "            factor_values[date] = top_mean - bottom_mean\n",
    "\n",
    "    # Create a DataFrame from the factor values\n",
    "    factor_df = pd.DataFrame.from_dict(factor_values, orient='index', columns=['DL_Momentum_Factor'])\n",
    "    factor_df.index.name = 'date'\n",
    "\n",
    "    return factor_df\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Suppose predictions_df is your DataFrame of LSTM encoder predictions:\n",
    "# predictions_df =\n",
    "#              AAPL    MSFT   GOOG   ...  ZTS\n",
    "# 2023-01-01   0.12    0.05   -0.03  ...  0.09\n",
    "# 2023-02-01   0.08    0.03    0.01  ...  0.10\n",
    "# ...\n",
    "#\n",
    "# Then compute the DL_Momentum_Factor as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaca7b5",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef752ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "DL_Factor=compute_DL_momentum_factor(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984bb62",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "DL_Factor_Norm=normalize_df(DL_Factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e501c",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Drop rows with missing values created during transformations\n",
    "market= ff_5_monthly[ff_5_monthly.columns[0]]\n",
    "# Use the first deep learning factor column.\n",
    "dl_factor = DL_Factor_Norm\n",
    "# Construct predictor DataFrame with constant.\n",
    "X = pd.concat([market, dl_factor], axis=1)\n",
    "X.columns = ['Mkt-RF', 'DL_Factor']\n",
    "X.dropna(inplace=True)\n",
    "#X = sm.add_constant(X).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e672798",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea22ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "# Plot results with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot each feature\n",
    "for column in X.columns:\n",
    "    plt.plot(X.index, X[column], label=column, linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Improve aesthetics\n",
    "plt.title('Regression Factors', fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Time\", fontsize=14)\n",
    "plt.ylabel(\"Feature Value\", fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Adjust the legend to fit inside the plot neatly\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1), ncol=2, fontsize=10, frameon=False)\n",
    "\n",
    "# Optimize layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf5b6c",
   "metadata": {},
   "source": [
    "### Imports & setup\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & setup ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def run_regressions_with_factors(target_set, ff_5_monthly, normalized_dl_factors):\n",
    "    \"\"\"\n",
    "    Runs a regression for each stock (column in target_set) using the market factor and\n",
    "    normalized deep learning factors as predictors. Returns a DataFrame summarizing the results.\n",
    "\n",
    "    Parameters:\n",
    "      target_set: pd.DataFrame\n",
    "          DataFrame of stock returns (index: dates, columns: stocks).\n",
    "      ff_5_monthly: pd.DataFrame\n",
    "          DataFrame with dates as index and a column 'Mkt-RF' for the market factor.\n",
    "      normalized_dl_factors: pd.DataFrame\n",
    "          DataFrame with dates as index and columns for the deep learning factors.\n",
    "\n",
    "    Returns:\n",
    "      results_df: pd.DataFrame\n",
    "          A DataFrame with one row per stock, containing the estimated coefficients,\n",
    "          their p-values, and the R-squared.\n",
    "    \"\"\"\n",
    "    # Align the indices: only keep dates common to all three DataFrames.\n",
    "    common_index = target_set.index.intersection(ff_5_monthly.index).intersection(normalized_dl_factors.index)\n",
    "    y_data = target_set.loc[common_index]\n",
    "    market = ff_5_monthly.loc[common_index, 'Mkt-RF']\n",
    "    dl_factors = normalized_dl_factors.loc[common_index]\n",
    "\n",
    "    # Combine predictors into one DataFrame.\n",
    "    # First column is the market factor, then the deep learning factors.\n",
    "    X = pd.concat([market, dl_factors], axis=1)\n",
    "    # Add constant term.\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for ticker in y_data.columns:\n",
    "        y = y_data[ticker]\n",
    "        # Run the regression. Use missing='drop' to handle any missing data.\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        # Build a dictionary to store results.\n",
    "        result_dict = {\n",
    "            'Ticker': ticker,\n",
    "            'Alpha': model.params.get('const', np.nan),\n",
    "            #'Alpha_pval': model.pvalues.get('const', np.nan),\n",
    "            'Market_Coeff': model.params.get('Mkt-RF', np.nan),\n",
    "            'Market_pval': model.pvalues.get('Mkt-RF', np.nan),\n",
    "            'R2': model.rsquared\n",
    "        }\n",
    "        # For each deep learning factor, store the coefficient and p-value.\n",
    "        for factor in dl_factors.columns:\n",
    "            result_dict[factor] = model.params.get(factor, np.nan)\n",
    "            result_dict[factor + '_pval'] = model.pvalues.get(factor, np.nan)\n",
    "\n",
    "        results_list.append(result_dict)\n",
    "\n",
    "    results_df = pd.DataFrame(results_list).set_index('Ticker')\n",
    "    return results_df\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume that target_set, ff_5_monthly, and normalized_dl_factors are pre-loaded DataFrames.\n",
    "# For instance:\n",
    "# target_set: shape (dates, 473)\n",
    "# ff_5_monthly: must have a column 'Mkt-RF'\n",
    "# normalized_dl_factors: shape (dates, n_factors)\n",
    "\n",
    "#results_df = run_regressions_with_factors(target_set, ff_5_monthly, DL_Factor_Norm)\n",
    "#print(\"Regression Results (first few rows):\")\n",
    "#print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff83ce",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "def run_regressions_two_factors(target_set, ff_5_monthly, normalized_dl_factors):\n",
    "    \"\"\"\n",
    "    For each stock in target_set, regress its returns on:\n",
    "      - A constant,\n",
    "      - The market factor (ff_5_monthly['Mkt-RF']),\n",
    "      - A deep learning factor (the first column of normalized_dl_factors).\n",
    "\n",
    "    Only dates common to all three DataFrames are used.\n",
    "\n",
    "    Parameters:\n",
    "      target_set: pd.DataFrame\n",
    "          DataFrame with dates as index and stock return series as columns.\n",
    "      ff_5_monthly: pd.DataFrame\n",
    "          DataFrame with dates as index and a column 'Mkt-RF' for the market factor.\n",
    "      normalized_dl_factors: pd.DataFrame\n",
    "          DataFrame with dates as index and one or more deep learning factor columns\n",
    "          (we will use the first column as our predictor).\n",
    "\n",
    "    Returns:\n",
    "      results_df: pd.DataFrame\n",
    "          A DataFrame with one row per stock that includes the estimated coefficients,\n",
    "          their p-values, and the R-squared.\n",
    "    \"\"\"\n",
    "    # Align the indices across the three DataFrames.\n",
    "    common_index = target_set.index.intersection(ff_5_monthly.index).intersection(normalized_dl_factors.index)\n",
    "    print(common_index)\n",
    "    y_data = target_set.loc[common_index]\n",
    "    market = ff_5_monthly.loc[common_index, 'Mkt-RF']\n",
    "    # Use the first deep learning factor column.\n",
    "    dl_factor = normalized_dl_factors\n",
    "\n",
    "    # Construct predictor DataFrame with constant.\n",
    "    X = pd.concat([market, dl_factor], axis=1)\n",
    "    X.columns = ['Mkt-RF', 'DL_Factor']\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for ticker in y_data.columns:\n",
    "        y = y_data[ticker]\n",
    "        model = sm.OLS(y, X, missing='drop').fit()\n",
    "        results_list.append({\n",
    "            'Ticker': ticker,\n",
    "            'Alpha': model.params.get('const', np.nan),\n",
    "            'Alpha_pval': model.pvalues.get('const', np.nan),\n",
    "            'Market_Coeff': model.params.get('Mkt-RF', np.nan),\n",
    "            'Market_pval': model.pvalues.get('Mkt-RF', np.nan),\n",
    "            'DL_Coeff': model.params.get('DL_Factor', np.nan),\n",
    "            'DL_pval': model.pvalues.get('DL_Factor', np.nan),\n",
    "            'R2': model.rsquared\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list).set_index('Ticker')\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9353bd0",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "target_set_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf060c",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "tickers=stock_returns['tickers'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ccd91d",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "ff_5_monthly.columns[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310ac70",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af43d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4713b8",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Drop rows with missing values created during transformations\n",
    "#ommon_index = target_set.index.intersection(ff_5_monthly.index).intersection(DL_Factor_Norm.index)\n",
    "y_data = target_set_test\n",
    "y_data.columns=tickers[:442]\n",
    "market= ff_5_monthly[ff_5_monthly.columns[0]]\n",
    "# Use the first deep learning factor column.\n",
    "dl_factor = DL_Factor_Norm\n",
    "# Construct predictor DataFrame with constant.\n",
    "X = pd.concat([market, dl_factor], axis=1)\n",
    "X.columns = ['Mkt-RF', 'DL_Factor']\n",
    "X = sm.add_constant(X).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27974971",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030d583",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e42f7c",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fa05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "X['DL_Factor'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de29015",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd647ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "# Drop rows with missing values created during transformations\n",
    "ic_values = []\n",
    "t_values = []\n",
    "rank_ic_values = []\n",
    "returns_df=y_data.loc[X.index,:]\n",
    "#returns_df = returns_data.dropna()\n",
    "dl_factor = np.array(dl_factor).flatten()\n",
    "\n",
    "\n",
    "for stock in returns_df.columns:\n",
    "    y = returns_df[stock].values.flatten()\n",
    "    X=X #Just for remebrance that it was already computed\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    ic_values.append(model.params['DL_Factor'])  # Store IC values\n",
    "    t_values.append(model.tvalues['DL_Factor'])  # Store t-values\n",
    "\n",
    "    # RankIC calculation using Spearman's rank correlation to handle non-linearity\n",
    "    if len(dl_factor) == len(y):\n",
    "        rank_ic_values.append(pd.Series(dl_factor).corr(y, method='spearman'))\n",
    "    else:\n",
    "        rank_ic_values.append(np.nan)\n",
    "\n",
    "# Convert lists to arrays for calculations\n",
    "ic_values = np.array(ic_values)\n",
    "t_values = np.array(t_values)\n",
    "rank_ic_values = np.array(rank_ic_values)\n",
    "\n",
    "# Compute required statistics\n",
    "ic_mean = np.mean(ic_values)\n",
    "ic_std = np.std(ic_values)\n",
    "icir = ic_mean / ic_std if ic_std != 0 else np.nan\n",
    "proportion_ic_gt_zero = np.mean(ic_values > 0) * 100\n",
    "proportion_ic_abs_gt_002 = np.mean(np.abs(ic_values) > 0.02) * 100\n",
    "\n",
    "rank_ic_mean = np.mean(rank_ic_values)\n",
    "rank_ic_std = np.std(rank_ic_values)\n",
    "rank_icir = rank_ic_mean / rank_ic_std if rank_ic_std != 0 else np.nan\n",
    "proportion_rank_ic_gt_zero = np.mean(rank_ic_values > 0) * 100\n",
    "proportion_rank_ic_abs_gt_002 = np.mean(np.abs(rank_ic_values) > 0.02) * 100\n",
    "\n",
    "t_value_mean = np.mean(np.abs(t_values))\n",
    "proportion_t_value_gt_1_96 = np.mean(np.abs(t_values) > 1.96) * 100\n",
    "\n",
    "# Creating the results DataFrame\n",
    "dl_factor_statistics = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Mean IC\", \"IC std\", \"ICIR\",\n",
    "        \"Proportion that IC > 0\", \"Proportion that |IC| > 0.02\",\n",
    "        \"Mean RankIC\", \"RankIC std\", \"RankICIR\",\n",
    "        \"Proportion that RankIC > 0\", \"Proportion that |RankIC| > 0.02\",\n",
    "        \"|t-value| mean\", \"Proportion that |t-value| > 1.96\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        ic_mean, ic_std, icir,\n",
    "        proportion_ic_gt_zero, proportion_ic_abs_gt_002,\n",
    "        rank_ic_mean, rank_ic_std, rank_icir,\n",
    "        proportion_rank_ic_gt_zero, proportion_rank_ic_abs_gt_002,\n",
    "        t_value_mean, proportion_t_value_gt_1_96\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display results\n",
    "print(dl_factor_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5c1af",
   "metadata": {},
   "source": [
    "### Modeling / regression\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Modeling / regression ===\n",
    "# Adjusting RankIC calculation to ensure proper type compatibility\n",
    "\n",
    "# Ensure the returns DataFrame matches the independent variables index\n",
    "returns_df = returns_df.loc[X.index, :]\n",
    "\n",
    "# Convert dl_factor to a Pandas Series to match types\n",
    "dl_factor_series = pd.Series(dl_factor.flatten(), index=X.index)\n",
    "\n",
    "# Initialize storage for IC, RankIC, and t-values\n",
    "ic_values = []\n",
    "t_values = []\n",
    "rank_ic_values = []\n",
    "\n",
    "for stock in returns_df.columns:\n",
    "    y = returns_df[stock]  # Ensure y is a Pandas Series with matching index\n",
    "    model = sm.OLS(y, X).fit()  # Perform regression\n",
    "\n",
    "    ic_values.append(model.params['DL_Factor'])  # Store IC values\n",
    "    t_values.append(model.tvalues['DL_Factor'])  # Store t-values\n",
    "\n",
    "    # Compute RankIC ensuring alignment of indices\n",
    "    y_series = pd.Series(y, index=X.index)\n",
    "    rank_ic_values.append(dl_factor_series.corr(y_series, method='spearman'))\n",
    "\n",
    "# Convert lists to arrays for calculations\n",
    "ic_values = np.array(ic_values)\n",
    "t_values = np.array(t_values)\n",
    "rank_ic_values = np.array(rank_ic_values)\n",
    "\n",
    "# Compute required statistics\n",
    "ic_mean = np.mean(ic_values)\n",
    "ic_std = np.std(ic_values)\n",
    "icir = ic_mean / ic_std if ic_std != 0 else np.nan\n",
    "proportion_ic_gt_zero = np.mean(ic_values > 0) * 100\n",
    "proportion_ic_abs_gt_002 = np.mean(np.abs(ic_values) > 0.02) * 100\n",
    "\n",
    "rank_ic_mean = np.nanmean(rank_ic_values)\n",
    "rank_ic_std = np.nanstd(rank_ic_values)\n",
    "rank_icir = rank_ic_mean / rank_ic_std if rank_ic_std != 0 else np.nan\n",
    "proportion_rank_ic_gt_zero = np.mean(rank_ic_values > 0) * 100\n",
    "proportion_rank_ic_abs_gt_002 = np.mean(np.abs(rank_ic_values) > 0.02) * 100\n",
    "\n",
    "t_value_mean = np.mean(np.abs(t_values))\n",
    "proportion_t_value_gt_1_96 = np.mean(np.abs(t_values) > 1.96) * 100\n",
    "\n",
    "# Creating the results DataFrame\n",
    "dl_factor_statistics = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Mean IC\", \"IC std\", \"ICIR\",\n",
    "        \"Proportion that IC > 0\", \"Proportion that |IC| > 0.02\",\n",
    "        \"Mean RankIC\", \"RankIC std\", \"RankICIR\",\n",
    "        \"Proportion that RankIC > 0\", \"Proportion that |RankIC| > 0.02\",\n",
    "        \"|t-value| mean\", \"Proportion that |t-value| > 1.96\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        ic_mean, ic_std, icir,\n",
    "        proportion_ic_gt_zero, proportion_ic_abs_gt_002,\n",
    "        rank_ic_mean, rank_ic_std, rank_icir,\n",
    "        proportion_rank_ic_gt_zero, proportion_rank_ic_abs_gt_002,\n",
    "        t_value_mean, proportion_t_value_gt_1_96\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the results DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd5eb7",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "dl_factor_statistics = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Mean IC\", \"IC std\", \"ICIR\",\n",
    "        \"Proportion that IC > 0 (%)\", \"Proportion that |IC| > 0.02 (%)\",\n",
    "        \"Mean RankIC\", \"RankIC std\", \"RankICIR\",\n",
    "        \"Proportion that RankIC > 0 (%)\", \"Proportion that |RankIC| > 0.02 (%)\",\n",
    "        \"|t-value| mean\", \"Proportion that |t-value| > 1.96 (%)\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        ic_mean, ic_std, icir,\n",
    "        f\"{proportion_ic_gt_zero:.2f}%\", f\"{proportion_ic_abs_gt_002:.2f}%\",\n",
    "        rank_ic_mean, rank_ic_std, rank_icir,\n",
    "        f\"{proportion_rank_ic_gt_zero:.2f}%\", f\"{proportion_rank_ic_abs_gt_002:.2f}%\",\n",
    "        t_value_mean, f\"{proportion_t_value_gt_1_96:.2f}%\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c603f4",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a883d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "dl_factor_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f70ce3",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7abe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization ===\n",
    "# Plot results with matplotlib\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create table plot\n",
    "table = ax.table(cellText=dl_factor_statistics.values,\n",
    "                 colLabels=dl_factor_statistics.columns,\n",
    "                 cellLoc='center', loc='center')\n",
    "\n",
    "# Style the table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width([0, 1])\n",
    "\n",
    "# Save the table as an image\n",
    "plt.savefig(\"deep_learning_factor_statistics_table.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the table\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b198bf9",
   "metadata": {},
   "source": [
    "### Clean & prepare data\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75191176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & prepare data ===\n",
    "results_list = []\n",
    "for ticker in y_data.columns:\n",
    "  try:\n",
    "    y = y_data.loc[X.index,ticker]\n",
    "    model = sm.OLS(y, X, missing='drop').fit()\n",
    "    results_list.append({\n",
    "        'Ticker': ticker,\n",
    "        'Alpha': model.params.get('const', np.nan),\n",
    "        'Alpha_pval': model.pvalues.get('const', np.nan),\n",
    "        'Market_Coeff': model.params.get('Mkt-RF', np.nan),\n",
    "        'Market_pval': model.pvalues.get('Mkt-RF', np.nan),\n",
    "        'DL_Coeff': model.params.get('DL_Factor', np.nan),\n",
    "        'DL_pval': model.pvalues.get('DL_Factor', np.nan),\n",
    "        'R2': model.rsquared\n",
    "        })\n",
    "  except Exception as e:\n",
    "      print(f\"Skipping ticker {ticker} due to error: {e}\")\n",
    "      continue\n",
    "\n",
    "  results_df = pd.DataFrame(results_list).set_index('Ticker')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffea0d",
   "metadata": {},
   "source": [
    "### Computation\n",
    "Briefly: what this step does and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Computation ===\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
